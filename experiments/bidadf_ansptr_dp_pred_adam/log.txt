Number of params: 873432 (retrieval took 4.675646 secs)
Beginning training loop...
epoch 1, iter 100, loss 8.24125, smoothed loss 9.03522, grad norm 3.22233, param norm 53.30441, batch time 2.552
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 200, loss 7.43364, smoothed loss 8.26179, grad norm 4.80705, param norm 55.00537, batch time 2.724
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 300, loss 7.52388, smoothed loss 7.60856, grad norm 5.45785, param norm 57.10692, batch time 2.507
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 400, loss 6.63353, smoothed loss 7.04302, grad norm 4.08853, param norm 59.03200, batch time 2.554
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 500, loss 6.17847, smoothed loss 6.67506, grad norm 4.51486, param norm 60.80827, batch time 2.680
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 600, loss 5.49387, smoothed loss 6.35547, grad norm 4.66422, param norm 62.52163, batch time 2.519
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 700, loss 6.02754, smoothed loss 6.05195, grad norm 4.92588, param norm 64.14948, batch time 2.585
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 800, loss 5.17794, smoothed loss 5.72941, grad norm 4.77755, param norm 65.62403, batch time 2.340
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 900, loss 5.58830, smoothed loss 5.42570, grad norm 5.53270, param norm 67.08082, batch time 2.553
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 1000, loss 4.69874, smoothed loss 5.20055, grad norm 4.70623, param norm 68.36061, batch time 2.485
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 1, Iter 1000, dev loss: 4.450154
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.86 seconds
Epoch 1, Iter 1000, Train F1 score: 0.498504, Train EM score: 0.371000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 164.55 seconds
Epoch 1, Iter 1000, Dev F1 score: 0.492160, Dev EM score: 0.340327
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 1, iter 1100, loss 4.57677, smoothed loss 4.99670, grad norm 4.82778, param norm 69.50459, batch time 2.406
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 1200, loss 4.22412, smoothed loss 4.85763, grad norm 4.47979, param norm 70.59914, batch time 2.452
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 1300, loss 4.11773, smoothed loss 4.68213, grad norm 5.07900, param norm 71.65920, batch time 2.324
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 1, iter 1400, loss 4.66285, smoothed loss 4.64986, grad norm 4.51737, param norm 72.65322, batch time 2.466
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 1. Time for epoch: 4080.703093
epoch 2, iter 1500, loss 4.90858, smoothed loss 4.53465, grad norm 5.60414, param norm 73.55665, batch time 2.460
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 1600, loss 4.11730, smoothed loss 4.43434, grad norm 4.35815, param norm 74.45911, batch time 2.377
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 1700, loss 4.07999, smoothed loss 4.38691, grad norm 4.26916, param norm 75.27707, batch time 2.579
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 1800, loss 3.90815, smoothed loss 4.33387, grad norm 4.36469, param norm 76.07919, batch time 2.564
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 1900, loss 4.65221, smoothed loss 4.31241, grad norm 4.50531, param norm 76.87674, batch time 2.493
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 2000, loss 4.90537, smoothed loss 4.24749, grad norm 4.64859, param norm 77.66798, batch time 2.532
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 2, Iter 2000, dev loss: 3.687865
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.75 seconds
Epoch 2, Iter 2000, Train F1 score: 0.634880, Train EM score: 0.478000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 165.45 seconds
Epoch 2, Iter 2000, Dev F1 score: 0.588652, Dev EM score: 0.427334
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 2, iter 2100, loss 4.35106, smoothed loss 4.15385, grad norm 5.32463, param norm 78.47394, batch time 2.418
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 2200, loss 3.83058, smoothed loss 4.06738, grad norm 4.58242, param norm 79.17526, batch time 2.314
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 2300, loss 3.96357, smoothed loss 4.13669, grad norm 4.30386, param norm 79.88142, batch time 2.499
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 2400, loss 4.65633, smoothed loss 4.06028, grad norm 4.24888, param norm 80.62920, batch time 2.636
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 2500, loss 4.64489, smoothed loss 4.05937, grad norm 4.95037, param norm 81.29659, batch time 2.497
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 2600, loss 3.81881, smoothed loss 3.96564, grad norm 4.72375, param norm 82.02596, batch time 2.415
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 2700, loss 3.92899, smoothed loss 3.92780, grad norm 4.61364, param norm 82.74462, batch time 2.473
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 2, iter 2800, loss 3.60906, smoothed loss 3.96602, grad norm 4.25462, param norm 83.41393, batch time 2.586
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 2. Time for epoch: 3982.080556
epoch 3, iter 2900, loss 3.10870, smoothed loss 3.93816, grad norm 4.14652, param norm 84.06680, batch time 2.345
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3000, loss 4.54889, smoothed loss 3.92650, grad norm 5.34177, param norm 84.72273, batch time 2.658
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 3, Iter 3000, dev loss: 3.473539
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.08 seconds
Epoch 3, Iter 3000, Train F1 score: 0.674937, Train EM score: 0.535000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 162.84 seconds
Epoch 3, Iter 3000, Dev F1 score: 0.613349, Dev EM score: 0.453224
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 3, iter 3100, loss 3.86234, smoothed loss 3.84495, grad norm 4.48924, param norm 85.32903, batch time 2.508
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3200, loss 3.63882, smoothed loss 3.86003, grad norm 3.87177, param norm 85.95123, batch time 2.298
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3300, loss 3.92225, smoothed loss 3.85393, grad norm 4.61899, param norm 86.58253, batch time 2.504
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3400, loss 3.70694, smoothed loss 3.80426, grad norm 4.29231, param norm 87.20123, batch time 2.278
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3500, loss 4.21094, smoothed loss 3.76916, grad norm 4.92795, param norm 87.83720, batch time 2.378
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3600, loss 3.46705, smoothed loss 3.74171, grad norm 4.46086, param norm 88.42607, batch time 2.618
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3700, loss 3.33395, smoothed loss 3.70408, grad norm 4.49851, param norm 89.02357, batch time 2.294
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3800, loss 3.78611, smoothed loss 3.71440, grad norm 4.10255, param norm 89.57364, batch time 2.503
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 3900, loss 3.76259, smoothed loss 3.71013, grad norm 4.25753, param norm 90.15929, batch time 2.291
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 4000, loss 3.12605, smoothed loss 3.75097, grad norm 4.21079, param norm 90.74032, batch time 2.471
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 3, Iter 4000, dev loss: 3.358280
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 22.53 seconds
Epoch 3, Iter 4000, Train F1 score: 0.664155, Train EM score: 0.527000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 161.93 seconds
Epoch 3, Iter 4000, Dev F1 score: 0.624171, Dev EM score: 0.470164
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 3, iter 4100, loss 3.67692, smoothed loss 3.68673, grad norm 4.43064, param norm 91.30888, batch time 2.341
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 4200, loss 3.49746, smoothed loss 3.66303, grad norm 4.52988, param norm 91.87886, batch time 2.769
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 3, iter 4300, loss 3.90744, smoothed loss 3.67957, grad norm 5.04041, param norm 92.43700, batch time 2.476
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 3. Time for epoch: 4319.597557
epoch 4, iter 4400, loss 3.55433, smoothed loss 3.65825, grad norm 4.47317, param norm 93.01943, batch time 2.873
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 4500, loss 3.85421, smoothed loss 3.63597, grad norm 4.48493, param norm 93.56250, batch time 2.886
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 4600, loss 3.68527, smoothed loss 3.64305, grad norm 3.75666, param norm 94.16166, batch time 2.379
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 4700, loss 3.73006, smoothed loss 3.67184, grad norm 4.03182, param norm 94.69582, batch time 2.674
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 4800, loss 3.54967, smoothed loss 3.62024, grad norm 4.03527, param norm 95.23868, batch time 2.427
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 4900, loss 3.17806, smoothed loss 3.59575, grad norm 3.86818, param norm 95.80019, batch time 2.477
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 5000, loss 4.10044, smoothed loss 3.56684, grad norm 4.00672, param norm 96.33556, batch time 2.315
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 4, Iter 5000, dev loss: 3.253109
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.11 seconds
Epoch 4, Iter 5000, Train F1 score: 0.705030, Train EM score: 0.561000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 163.19 seconds
Epoch 4, Iter 5000, Dev F1 score: 0.635650, Dev EM score: 0.478922
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 4, iter 5100, loss 4.30287, smoothed loss 3.53416, grad norm 4.56488, param norm 96.86028, batch time 2.435
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 5200, loss 3.11892, smoothed loss 3.53777, grad norm 4.59802, param norm 97.33753, batch time 2.558
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 5300, loss 3.91316, smoothed loss 3.58749, grad norm 4.40178, param norm 97.86462, batch time 2.782
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 5400, loss 2.45489, smoothed loss 3.54160, grad norm 4.32391, param norm 98.35052, batch time 2.403
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 5500, loss 2.90339, smoothed loss 3.51914, grad norm 3.52548, param norm 98.86306, batch time 2.388
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 5600, loss 3.18115, smoothed loss 3.47580, grad norm 4.01333, param norm 99.39803, batch time 2.381
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 4, iter 5700, loss 3.02726, smoothed loss 3.50549, grad norm 4.19727, param norm 99.87096, batch time 2.365
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 4. Time for epoch: 3978.239885
epoch 5, iter 5800, loss 2.91993, smoothed loss 3.50523, grad norm 3.60347, param norm 100.34650, batch time 2.383
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 5900, loss 3.83575, smoothed loss 3.48439, grad norm 4.68100, param norm 100.84502, batch time 2.522
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6000, loss 2.73166, smoothed loss 3.44732, grad norm 3.76378, param norm 101.35627, batch time 2.303
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 5, Iter 6000, dev loss: 3.227548
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.56 seconds
Epoch 5, Iter 6000, Train F1 score: 0.719040, Train EM score: 0.585000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 164.56 seconds
Epoch 5, Iter 6000, Dev F1 score: 0.638856, Dev EM score: 0.487199
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 5, iter 6100, loss 3.15880, smoothed loss 3.48735, grad norm 4.04422, param norm 101.81449, batch time 2.379
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6200, loss 4.46332, smoothed loss 3.47483, grad norm 5.00718, param norm 102.30634, batch time 2.272
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6300, loss 4.17243, smoothed loss 3.43596, grad norm 4.42467, param norm 102.79937, batch time 2.388
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6400, loss 2.77455, smoothed loss 3.42091, grad norm 3.72513, param norm 103.29654, batch time 2.573
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6500, loss 2.84309, smoothed loss 3.39934, grad norm 4.35083, param norm 103.75431, batch time 2.383
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6600, loss 3.83099, smoothed loss 3.41082, grad norm 4.72147, param norm 104.25362, batch time 2.530
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6700, loss 3.36099, smoothed loss 3.36945, grad norm 4.25207, param norm 104.73833, batch time 2.377
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6800, loss 3.75403, smoothed loss 3.39346, grad norm 5.03912, param norm 105.20546, batch time 2.478
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 6900, loss 3.26877, smoothed loss 3.42567, grad norm 4.40078, param norm 105.65294, batch time 2.504
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 5, iter 7000, loss 3.66674, smoothed loss 3.39608, grad norm 4.06693, param norm 106.12546, batch time 2.330
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 5, Iter 7000, dev loss: 3.141374
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.79 seconds
Epoch 5, Iter 7000, Train F1 score: 0.727150, Train EM score: 0.584000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 165.36 seconds
Epoch 5, Iter 7000, Dev F1 score: 0.650433, Dev EM score: 0.493840
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 5, iter 7100, loss 3.54551, smoothed loss 3.38965, grad norm 4.54237, param norm 106.55553, batch time 2.430
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 5. Time for epoch: 4320.253992
epoch 6, iter 7200, loss 3.40984, smoothed loss 3.39918, grad norm 4.60349, param norm 107.00433, batch time 2.303
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 7300, loss 3.90957, smoothed loss 3.38300, grad norm 4.03359, param norm 107.44126, batch time 2.415
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 7400, loss 2.58383, smoothed loss 3.35475, grad norm 3.81715, param norm 107.91043, batch time 2.746
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 7500, loss 3.11467, smoothed loss 3.36373, grad norm 4.14524, param norm 108.35033, batch time 2.682
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 7600, loss 3.66923, smoothed loss 3.36614, grad norm 4.33199, param norm 108.77394, batch time 2.527
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 7700, loss 3.22856, smoothed loss 3.35596, grad norm 4.22877, param norm 109.22775, batch time 2.299
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 7800, loss 3.54660, smoothed loss 3.36907, grad norm 4.33687, param norm 109.69178, batch time 2.481
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 7900, loss 3.68385, smoothed loss 3.31206, grad norm 4.71351, param norm 110.08824, batch time 2.470
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 8000, loss 3.40092, smoothed loss 3.31458, grad norm 4.87804, param norm 110.52390, batch time 2.542
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 6, Iter 8000, dev loss: 3.145483
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.74 seconds
Epoch 6, Iter 8000, Train F1 score: 0.721878, Train EM score: 0.574000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 164.53 seconds
Epoch 6, Iter 8000, Dev F1 score: 0.649280, Dev EM score: 0.495669
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 6, iter 8100, loss 3.53253, smoothed loss 3.30699, grad norm 5.01980, param norm 110.97415, batch time 2.460
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 8200, loss 3.13517, smoothed loss 3.33510, grad norm 4.18314, param norm 111.37454, batch time 2.245
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 8300, loss 2.93176, smoothed loss 3.32758, grad norm 4.08327, param norm 111.79213, batch time 2.532
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 8400, loss 3.41860, smoothed loss 3.29174, grad norm 4.34054, param norm 112.22235, batch time 2.599
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 8500, loss 2.77231, smoothed loss 3.33132, grad norm 4.00397, param norm 112.64492, batch time 2.599
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 6, iter 8600, loss 2.98570, smoothed loss 3.28836, grad norm 4.05178, param norm 113.05344, batch time 2.357
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 6. Time for epoch: 3977.786197
epoch 7, iter 8700, loss 2.78678, smoothed loss 3.30140, grad norm 4.24687, param norm 113.44341, batch time 2.375
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 8800, loss 3.03214, smoothed loss 3.27404, grad norm 3.90275, param norm 113.83202, batch time 2.409
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 8900, loss 3.07878, smoothed loss 3.30885, grad norm 4.16254, param norm 114.22284, batch time 2.290
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9000, loss 2.91820, smoothed loss 3.30019, grad norm 4.50837, param norm 114.64196, batch time 2.465
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 7, Iter 9000, dev loss: 3.135780
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.83 seconds
Epoch 7, Iter 9000, Train F1 score: 0.734801, Train EM score: 0.583000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 164.25 seconds
Epoch 7, Iter 9000, Dev F1 score: 0.651789, Dev EM score: 0.499326
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 7, iter 9100, loss 2.74272, smoothed loss 3.28633, grad norm 4.48752, param norm 115.06267, batch time 2.507
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9200, loss 3.18464, smoothed loss 3.28017, grad norm 3.76523, param norm 115.47858, batch time 2.662
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9300, loss 3.65956, smoothed loss 3.24171, grad norm 4.55204, param norm 115.87582, batch time 2.287
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9400, loss 2.78567, smoothed loss 3.21984, grad norm 3.93941, param norm 116.25220, batch time 2.386
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9500, loss 3.26748, smoothed loss 3.18352, grad norm 3.78705, param norm 116.67749, batch time 2.528
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9600, loss 2.84876, smoothed loss 3.25684, grad norm 3.61770, param norm 117.10743, batch time 2.572
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9700, loss 2.82436, smoothed loss 3.25243, grad norm 3.77155, param norm 117.51801, batch time 2.422
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9800, loss 2.69528, smoothed loss 3.21246, grad norm 4.08134, param norm 117.88803, batch time 2.420
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 9900, loss 2.80193, smoothed loss 3.21642, grad norm 4.04598, param norm 118.31227, batch time 2.402
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 7, iter 10000, loss 3.09348, smoothed loss 3.23707, grad norm 4.34995, param norm 118.70763, batch time 2.312
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 7, Iter 10000, dev loss: 3.137529
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.35 seconds
Epoch 7, Iter 10000, Train F1 score: 0.711617, Train EM score: 0.569000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 164.86 seconds
Epoch 7, Iter 10000, Dev F1 score: 0.651977, Dev EM score: 0.497594
End of epoch 7. Time for epoch: 4314.946252
epoch 8, iter 10100, loss 3.31269, smoothed loss 3.26738, grad norm 4.18104, param norm 119.07748, batch time 2.471
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 10200, loss 3.22053, smoothed loss 3.21327, grad norm 4.44133, param norm 119.46297, batch time 2.456
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 10300, loss 3.17871, smoothed loss 3.19474, grad norm 4.94816, param norm 119.86040, batch time 2.428
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 10400, loss 2.41884, smoothed loss 3.18969, grad norm 3.86897, param norm 120.24486, batch time 2.304
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 10500, loss 3.09857, smoothed loss 3.20910, grad norm 4.51570, param norm 120.63332, batch time 2.348
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 10600, loss 2.75876, smoothed loss 3.20767, grad norm 3.70096, param norm 121.05521, batch time 2.252
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 10700, loss 2.92751, smoothed loss 3.17398, grad norm 4.18274, param norm 121.42342, batch time 2.372
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 10800, loss 2.92924, smoothed loss 3.17849, grad norm 4.57547, param norm 121.79462, batch time 2.460
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 10900, loss 3.43458, smoothed loss 3.17667, grad norm 4.37862, param norm 122.15041, batch time 2.460
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 11000, loss 2.76751, smoothed loss 3.15472, grad norm 4.18638, param norm 122.52739, batch time 2.627
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 8, Iter 11000, dev loss: 3.093261
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.74 seconds
Epoch 8, Iter 11000, Train F1 score: 0.714271, Train EM score: 0.568000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 165.29 seconds
Epoch 8, Iter 11000, Dev F1 score: 0.656928, Dev EM score: 0.505486
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 8, iter 11100, loss 3.22601, smoothed loss 3.17153, grad norm 4.21821, param norm 122.91039, batch time 2.476
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 11200, loss 3.59796, smoothed loss 3.16023, grad norm 5.03947, param norm 123.30212, batch time 2.331
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 11300, loss 3.04158, smoothed loss 3.12460, grad norm 4.45280, param norm 123.66238, batch time 2.655
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 8, iter 11400, loss 3.09825, smoothed loss 3.17630, grad norm 4.09735, param norm 124.02393, batch time 2.334
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 8. Time for epoch: 3978.150346
epoch 9, iter 11500, loss 3.48614, smoothed loss 3.20235, grad norm 4.42910, param norm 124.39751, batch time 2.611
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 11600, loss 3.24254, smoothed loss 3.19728, grad norm 4.36950, param norm 124.74500, batch time 2.278
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 11700, loss 2.91472, smoothed loss 3.19722, grad norm 3.98956, param norm 125.12727, batch time 2.445
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 11800, loss 3.19188, smoothed loss 3.13367, grad norm 4.79391, param norm 125.49123, batch time 2.551
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 11900, loss 2.47666, smoothed loss 3.19227, grad norm 3.88790, param norm 125.83750, batch time 2.503
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12000, loss 3.77073, smoothed loss 3.17307, grad norm 4.49405, param norm 126.18731, batch time 2.371
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 9, Iter 12000, dev loss: 3.069815
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.70 seconds
Epoch 9, Iter 12000, Train F1 score: 0.726099, Train EM score: 0.579000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 163.96 seconds
Epoch 9, Iter 12000, Dev F1 score: 0.663631, Dev EM score: 0.507026
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 9, iter 12100, loss 3.66797, smoothed loss 3.14735, grad norm 5.12394, param norm 126.53947, batch time 2.352
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12200, loss 3.93996, smoothed loss 3.09821, grad norm 4.89941, param norm 126.88964, batch time 2.295
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12300, loss 2.60714, smoothed loss 3.12661, grad norm 3.64859, param norm 127.20029, batch time 2.490
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12400, loss 3.56722, smoothed loss 3.16193, grad norm 4.16704, param norm 127.54263, batch time 2.494
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12500, loss 3.50958, smoothed loss 3.15448, grad norm 4.63159, param norm 127.90313, batch time 2.337
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12600, loss 2.00758, smoothed loss 3.15820, grad norm 3.34648, param norm 128.26199, batch time 2.435
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12700, loss 3.64951, smoothed loss 3.11407, grad norm 4.65076, param norm 128.62955, batch time 2.313
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12800, loss 4.03020, smoothed loss 3.15759, grad norm 4.62503, param norm 128.97437, batch time 2.366
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 9, iter 12900, loss 3.15181, smoothed loss 3.14326, grad norm 4.32955, param norm 129.32918, batch time 2.295
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 9. Time for epoch: 3978.214952
epoch 10, iter 13000, loss 2.50749, smoothed loss 3.15364, grad norm 3.83099, param norm 129.66020, batch time 2.425
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 10, Iter 13000, dev loss: 3.066393
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.90 seconds
Epoch 10, Iter 13000, Train F1 score: 0.762812, Train EM score: 0.622000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 164.65 seconds
Epoch 10, Iter 13000, Dev F1 score: 0.662852, Dev EM score: 0.509625
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/best_checkpoint/qa_best.ckpt...
epoch 10, iter 13100, loss 2.58372, smoothed loss 3.09106, grad norm 3.94646, param norm 130.00510, batch time 2.515
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 13200, loss 3.17244, smoothed loss 3.13461, grad norm 4.64793, param norm 130.37268, batch time 2.490
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 13300, loss 3.32710, smoothed loss 3.11780, grad norm 4.93175, param norm 130.71219, batch time 2.326
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 13400, loss 3.51933, smoothed loss 3.12584, grad norm 4.41022, param norm 131.04921, batch time 2.532
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 13500, loss 3.28582, smoothed loss 3.09534, grad norm 4.68399, param norm 131.37643, batch time 2.328
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 13600, loss 2.65338, smoothed loss 3.05450, grad norm 4.07356, param norm 131.72926, batch time 2.325
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 13700, loss 3.13029, smoothed loss 3.08783, grad norm 4.38923, param norm 132.04282, batch time 2.288
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 13800, loss 2.41759, smoothed loss 3.11770, grad norm 3.86375, param norm 132.39156, batch time 2.542
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 13900, loss 3.73546, smoothed loss 3.08766, grad norm 4.34260, param norm 132.72980, batch time 2.447
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 14000, loss 3.68759, smoothed loss 3.15352, grad norm 4.31334, param norm 133.06013, batch time 2.395
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 10, Iter 14000, dev loss: 3.034026
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 23.68 seconds
Epoch 10, Iter 14000, Train F1 score: 0.732262, Train EM score: 0.593000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 165.04 seconds
Epoch 10, Iter 14000, Dev F1 score: 0.666381, Dev EM score: 0.509336
epoch 10, iter 14100, loss 2.78537, smoothed loss 3.06293, grad norm 4.62298, param norm 133.36974, batch time 2.354
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 14200, loss 3.39251, smoothed loss 3.09285, grad norm 4.71635, param norm 133.69232, batch time 2.477
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 10, iter 14300, loss 2.58295, smoothed loss 3.07767, grad norm 4.07570, param norm 133.99954, batch time 2.404
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
End of epoch 10. Time for epoch: 4322.598961
epoch 11, iter 14400, loss 3.40499, smoothed loss 3.08682, grad norm 4.57217, param norm 134.32188, batch time 2.479
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 14500, loss 2.75366, smoothed loss 3.08227, grad norm 3.68649, param norm 134.65479, batch time 2.349
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 14600, loss 3.18820, smoothed loss 3.10225, grad norm 4.62911, param norm 134.96336, batch time 2.806
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 14700, loss 3.06671, smoothed loss 3.09472, grad norm 3.90432, param norm 135.30923, batch time 2.378
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 14800, loss 3.49320, smoothed loss 3.08556, grad norm 5.05981, param norm 135.61664, batch time 2.361
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 14900, loss 2.68318, smoothed loss 3.03921, grad norm 4.18820, param norm 135.93695, batch time 2.383
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 15000, loss 2.89491, smoothed loss 3.07791, grad norm 4.36591, param norm 136.25244, batch time 2.421
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
Calculating dev loss...
Epoch 11, Iter 15000, dev loss: 3.073079
Calculating F1/EM for 1000 examples in train set...
Calculating F1/EM for 1000 examples in train set took 22.58 seconds
Epoch 11, Iter 15000, Train F1 score: 0.753871, Train EM score: 0.611000
Calculating F1/EM for all examples in dev set...
Calculating F1/EM for 10390 examples in dev set took 166.04 seconds
Epoch 11, Iter 15000, Dev F1 score: 0.658364, Dev EM score: 0.509432
epoch 11, iter 15100, loss 2.80721, smoothed loss 3.06902, grad norm 4.64728, param norm 136.55624, batch time 2.314
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 15200, loss 3.46167, smoothed loss 3.05502, grad norm 4.43109, param norm 136.86717, batch time 2.352
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 15300, loss 2.83867, smoothed loss 3.05562, grad norm 4.32542, param norm 137.18440, batch time 2.392
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
epoch 11, iter 15400, loss 3.38611, smoothed loss 3.10559, grad norm 4.51820, param norm 137.48611, batch time 2.424
Saving to ../experiments/bidadf_ansptr_dp_pred_adam/qa.ckpt...
